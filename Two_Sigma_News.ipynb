{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "from kaggle.competitions import twosigmanews\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection  import train_test_split\n",
    "import lightgbm as lgb\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "317813c7689336abd6030753fc8257b783606b2f"
   },
   "source": [
    "### Import the environment and datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = twosigmanews.make_env()\n",
    "\n",
    "(market_train_df, news_train_df) = env.get_training_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4c9aa0ed01274c0de66d3a5f9dab29571378fcff"
   },
   "source": [
    "### Clean and reshape the data for use in model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4553df8864479f8e5af87880fcb882fa50cf022c"
   },
   "source": [
    "#### Remove data from the financial crisis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ff5f1b81ae6d45c03fe0928fa62d9edc9f5273fc",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_df_by_year(df):\n",
    "    df['date'] = df['time'].dt.floor('D')\n",
    "    df.set_index('date', inplace=True)\n",
    "    df = df[(df.index >= '2010-01-01') & (df.index <= '2016-12-31')]\n",
    "    df.reset_index(inplace=True)\n",
    "    return df\n",
    "\n",
    "market = split_df_by_year(market_train_df)\n",
    "news = split_df_by_year(news_train_df)\n",
    "\n",
    "del market_train_df\n",
    "del news_train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fe28590e239d3964166cad3e3e3da04a19796e9f"
   },
   "source": [
    "#### Market dataframe:\n",
    "#### 1. Drop rows with NANs in the market dataframe.\n",
    "#### 2. To be able to merge on the column later, rename the 'assetCode' column of the market dataframe to 'assetCodes'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "20ca3c41536d8543a3e235f1e2c69534b67a614b",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def market_shape(df,dropna):\n",
    "    if dropna:\n",
    "        df.dropna(inplace=True)\n",
    "    #df.rename(index=str, columns={\"assetCode\": \"assetCodes\"}, inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "312026ef98e5022be3e92403374350166305d8d7"
   },
   "source": [
    "#### News dataframe:\n",
    "#### 1. Drop columns that won't be used in the model to save memory.\n",
    "#### 2. The news dataframe has a column with multiple asset codes in one cell. Reshape the 'assetCodes' column of the news dataframe to expand each asset code into its own cell.\n",
    "#### 3. To be able to merge on this column later, strip extra characters from the news dataframe 'assetCodes' column.\n",
    "#### 4. Engineer features for proportion of words relevant to the asset and the position of the first mention of the asset in the news item.\n",
    "#### 5. Aggegate the data from the news dataframe on assetCode and date, and create columns for statistics of the aggregated data.\n",
    "#### 6. Format the aggregated columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0187b61e0034d65367a07ba5f4bb9b8bd4cbc0ef",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def news_shape(df):\n",
    "    df.drop(['sourceTimestamp', 'firstCreated', 'sourceId', 'headline', 'provider', 'subjects', 'audiences'], axis=1, inplace=True)\n",
    "    \n",
    "    #df = df.set_index(df.columns.drop('assetCodes',1).tolist()).assetCodes.str.split(' ', expand=True).stack().reset_index().rename(columns={0:'assetCodes'}).loc[:, df.columns]\n",
    "    \n",
    "    #df['assetCodes'] = df['assetCodes'].map(lambda x: x.lstrip('{' '\\'').rstrip('}' '\\'' ','))\n",
    "\n",
    "    df['proportionRelevant'] = df['sentimentWordCount'] / df['wordCount']\n",
    "\n",
    "    df['firstMentionPosition'] = df['firstMentionSentence'] / df['sentenceCount']\n",
    "    \n",
    "    df = df.groupby(['assetName', 'date']).agg({'urgency': [np.min, np.sum], 'takeSequence': np.max,\n",
    "                    'companyCount': [np.min, np.max, np.mean, np.std],\n",
    "                    'bodySize': [np.mean, np.std],\n",
    "                    'wordCount': [np.mean, np.std],\n",
    "                    'marketCommentary': [np.mean, np.std],\n",
    "                    'sentimentClass' : [np.sum, np.mean, np.std],\n",
    "                    'sentimentNegative': [np.mean, np.std],\n",
    "                    'sentimentNeutral': [np.mean, np.std],\n",
    "                    'sentimentPositive': [np.mean, np.std], \n",
    "                    'relevance' : [np.mean, np.std], \n",
    "                    'proportionRelevant' : [np.mean, np.std],\n",
    "                    'firstMentionPosition' : [np.mean, np.std],\n",
    "                    'noveltyCount12H' : [np.mean, np.std], \n",
    "                    'noveltyCount24H' : [np.mean, np.std],\n",
    "                    'noveltyCount3D' : [np.mean, np.std], \n",
    "                    'noveltyCount5D' : [np.mean, np.std],\n",
    "                    'noveltyCount7D' : [np.mean, np.std],\n",
    "                    'volumeCounts12H' : [np.mean, np.std],\n",
    "                    'volumeCounts24H' : [np.mean, np.std],\n",
    "                    'volumeCounts3D' : [np.mean, np.std],\n",
    "                    'volumeCounts5D' : [np.mean, np.std], \n",
    "                    'volumeCounts7D': [np.mean, np.std]})\n",
    "    \n",
    "    df = df.apply(np.float32)\n",
    "    \n",
    "    df.columns = ['_'.join(col).strip() for col in df.columns.values]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "67451382674d123c57ce048c4ca47998662c2498",
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "news.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "72b2539f9957a0674c0964c658429fdf36b3a5ff"
   },
   "source": [
    "#### Merge the market and news dataframes on asset code and date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "370872d3f61dd36739563c1de71b6e2122ec3a4e",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def merge(df1, df2):\n",
    "    df = pd.merge(df1, df2, how='left', on =['assetName', 'date'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f37711fc706da70d3411acd7c6cfd4c74aaddd7a"
   },
   "source": [
    "#### Shape and merge the data.\n",
    "#### Encode the assetCodes column as a unique numerical key to be used as a categorical feature in the model.\n",
    "#### Encode the date column as catergorical features to be used in the model.\n",
    "#### Remove all non-numerical data from the X dataframe.\n",
    "#### Separate the target feature (10 day market residualized returns) into the y dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d62fd52df437999c6160687ac52495df932bff05",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data(market, news):\n",
    "    data = shape_and_merge(market, news)\n",
    "    X = data\n",
    "    y = data['returnsOpenNextMktres10'].clip(-1, 1)\n",
    "    X.drop(columns=['returnsOpenNextMktres10'], inplace=True)\n",
    "    return X, y, data\n",
    "\n",
    "def label_encode(series, min_count):\n",
    "    vc = series.value_counts()\n",
    "    le = {c:i for i, c in enumerate(vc.index[vc >= min_count])}\n",
    "    return le\n",
    "\n",
    "def shape_and_merge(market, news, dropna=True):\n",
    "    market_shaped = market_shape(market, dropna)\n",
    "    news_shaped = news_shape(news)\n",
    "    df = merge(market_shaped, news_shaped)\n",
    "    \n",
    "    le = None\n",
    "    if le is None:\n",
    "        le_assetCode = label_encode(df['assetName'], min_count=10)\n",
    "    else:\n",
    "        le_assetCode = le\n",
    "    \n",
    "    df['assetName'] = df['assetName'].map(le_assetCode).fillna(-1).astype(int)\n",
    "    df['dayofweek'], df['month'] = df.time.dt.dayofweek, df.time.dt.month\n",
    "    return df\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "806d7e57fa010a423f12866db327a7ea6ca990c4",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, y, data = get_data(market, news)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1a2683ca4edce4d62f180561ff499af7d113a63c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X.shape, y.shape, data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ff74687a038e6c8057d16f301d533c74cf1d8d8e",
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ecb99dc21c9e2008ce2639434dc41d35d5e159c0"
   },
   "source": [
    "### Preprocess the data into training and validation sets and scale features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7803f71899aff7bbdfcfbb6cb107862ded7e1185",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "universe = data['universe']\n",
    "time = data['time']\n",
    "\n",
    "X.drop(['time', 'date', 'assetCode', 'universe'], axis=1, inplace=True)\n",
    "n_train = int(X.shape[0] * 0.95)\n",
    "\n",
    "X_train, y_train = X.iloc[:n_train], y.iloc[:n_train]\n",
    "X_valid, y_valid = X.iloc[n_train:], y.iloc[n_train:]\n",
    "\n",
    "u_valid = (universe.iloc[n_train:] > 0)\n",
    "t_valid = time.iloc[n_train:]\n",
    "\n",
    "X_valid = X_valid[u_valid]\n",
    "y_valid = y_valid[u_valid]\n",
    "t_valid = t_valid[u_valid]\n",
    "del u_valid\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "77521547779cedda7084b0a2ddc56cfacc23670b",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_valid = sc.transform(X_valid)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0030a563be37357f6e390f170ae0e662b6e9a148"
   },
   "source": [
    "### Convert the datasets to LGB format, train the model, and calculate the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "60a9c3a19c8b93293f4567a85f68e2e16bca0188",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert the datasets to LGB format\n",
    "train_cols = X.columns.tolist()\n",
    "categorical_cols = ['assetName', 'dayofweek', 'month'] \n",
    "\n",
    "# Note: y data is expected to be a pandas Series, as we will use its group_by function in `sigma_score`\n",
    "dtrain = lgb.Dataset(X_train, y_train, feature_name=train_cols, categorical_feature=categorical_cols, free_raw_data=False)\n",
    "dvalid = lgb.Dataset(X_valid, y_valid, feature_name=train_cols, categorical_feature=categorical_cols, free_raw_data=False)\n",
    "\n",
    "dvalid.params = {\n",
    "    'extra_time': t_valid.factorize()[0]\n",
    "}\n",
    "\n",
    "lgb_params = dict(\n",
    "    objective = 'regression_l1',\n",
    "    learning_rate = 0.05,\n",
    "    num_leaves = 125,\n",
    "    max_depth = -1,\n",
    "    min_data_in_leaf = 1000,\n",
    "    bagging_fraction = 0.75,\n",
    "    bagging_freq = 2,\n",
    "    feature_fraction = 0.5,\n",
    "    lambda_l1 = 0.0,\n",
    "    lambda_l2 = 1.0,\n",
    "    max_bin = 125,\n",
    "    metric = 'None', \n",
    "    seed = 0 \n",
    ")\n",
    "\n",
    "def score(preds, valid_data):\n",
    "    df_time = valid_data.params['extra_time']\n",
    "    labels = valid_data.get_label()\n",
    "    \n",
    "    x_t = preds * labels \n",
    "    x_t_sum = x_t.groupby(df_time).sum()\n",
    "    score = x_t_sum.mean() / x_t_sum.std()\n",
    "\n",
    "    return 'score', score, True\n",
    "\n",
    "evals_result = {}\n",
    "m = lgb.train(lgb_params, dtrain, num_boost_round=1000, valid_sets=(dvalid,), valid_names=('valid',), verbose_eval=25,\n",
    "              early_stopping_rounds=100, feval=score, evals_result=evals_result)\n",
    "\n",
    "df_result = pd.DataFrame(evals_result['valid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ab6d44a6c53a81c0a835de0c39725d5cf7c73fde",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ax = df_result.plot(figsize=(12, 8))\n",
    "ax.scatter(df_result['score'].idxmax(), df_result['score'].max(), marker='+', color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a5bd69599423116bb3bd550f3a44479d8a051994",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 14))\n",
    "lgb.plot_importance(m, ax=ax[0])\n",
    "lgb.plot_importance(m, ax=ax[1], importance_type='gain')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c8e802dc81fb2d42d9a15c1d297308fdfc2c62e7"
   },
   "source": [
    "### Make predictions and submit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8b39a405b273410d611b09d819cf15f9b4b7aec5",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "days = env.get_prediction_days()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "69e4d075cd9dad846a80ebcd6c0ec67dd1fa63ff",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_predictions(market_obs_df, news_obs_df, predictions_template_df):\n",
    "    market_obs_df['date'] = market_obs_df['time'].dt.floor('D')\n",
    "    news_obs_df['date'] = news_obs_df['time'].dt.floor('D')\n",
    "    x = shape_and_merge(market_obs_df, news_obs_df, False)\n",
    "    x.drop(['time', 'date', 'assetCode'], axis=1, inplace=True)\n",
    "    y = np.clip(m.predict(x), -1, 1)\n",
    "    print(y.shape, x.shape, predictions_template_df.shape)\n",
    "    predictions_template_df.confidenceValue = y\n",
    "    return predictions_template_df.confidenceValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0abec4d8245dc6d715cc938228141c4423565ff1",
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "for (market_obs_df, news_obs_df, predictions_template_df) in days:\n",
    "    predictions_template_df.head()\n",
    "    i += 1\n",
    "    print(i)\n",
    "    make_predictions(market_obs_df, news_obs_df, predictions_template_df)\n",
    "    env.predict(predictions_template_df)\n",
    "print('Done!')\n",
    "\n",
    "env.write_submission_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fbb3df5fdad96339d46aed085afed25e88d22d8b"
   },
   "source": [
    "#### Resources:\n",
    "[A Simple Model Using the Market and News Data](http://www.kaggle.com/bguberfain/a-simple-model-using-the-market-and-news-data/notebook)\n",
    "\n",
    "[What is LightGBM, How to implement it? How to fine tune the parameters?](http://https://medium.com/@pushkarmandot/https-medium-com-pushkarmandot-what-is-lightgbm-how-to-implement-it-how-to-fine-tune-the-parameters-60347819b7fc)\n",
    "\n",
    "[Light GBM docs](http://media.readthedocs.org/pdf/lightgbm/latest/lightgbm.pdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "511f3a7f2e8b023e9565ba37587a73e454796078",
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
